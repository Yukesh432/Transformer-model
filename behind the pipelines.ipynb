{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Behind the pipelines , there are 3 major steps:\n",
    "\n",
    "1. Tokenizer\n",
    "2. Model\n",
    "3. Postprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Tokenizer\n",
    "The first step of pipeline is to convert the text inputs into numbers that the model can make sense of it\n",
    "\n",
    "Tokenizer does the following:\n",
    "a. Splitting the input into words, subwords, or symbols called tokens\n",
    "b. Mapping each token to an integer\n",
    "c. Adding additional inputs that may be useful to the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[  101,  1045,  1005,  2310,  2042, 16054,  2075,  3330,  1998,  1045,\n",
      "          2123,  2102,  2123,  2102,  2113,  2339,   102],\n",
      "        [  101,  1045,  2123,  2102,  3305,  2122,  3395,  1012,  2049,  2061,\n",
      "          8552,   102,     0,     0,     0,     0,     0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0]])}\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "checkpoint= \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
    "tokenizer= AutoTokenizer.from_pretrained(checkpoint)\n",
    "\n",
    "#once we have the tokenizer , we can directly pass our sentences to it and get back a dictionary\n",
    "#we can pass one sentence or a list of sentences\n",
    "raw_inputs= [\n",
    "    \"I've been studing engineering and i dont dont know why\",\n",
    "    \"I dont understand these subject. Its so complicated\"]\n",
    "\n",
    "inputs= tokenizer(raw_inputs, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "print(inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Model\n",
    "\n",
    "We can download pretrained model same as tokenizer.\n",
    "For each model input, weâ€™ll retrieve a high-dimensional vector representing the contextual understanding of that input by the Transformer model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilbert-base-uncased-finetuned-sst-2-english were not used when initializing DistilBertModel: ['classifier.weight', 'pre_classifier.weight', 'pre_classifier.bias', 'classifier.bias']\n",
      "- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModel\n",
    "\n",
    "checkpoint= \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
    "model= AutoModel.from_pretrained(checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BaseModelOutput(last_hidden_state=tensor([[[-0.0869,  0.8823, -0.4551,  ...,  0.1345, -0.2991,  0.0832],\n",
      "         [-0.1462,  0.8814, -0.2718,  ..., -0.0106, -0.2913,  0.1056],\n",
      "         [-0.0470,  0.5186, -0.2083,  ..., -0.0832, -0.5778, -0.5181],\n",
      "         ...,\n",
      "         [-0.5489,  0.9351, -0.1455,  ..., -0.3314, -0.5637, -0.0822],\n",
      "         [-0.7415,  0.9158, -0.6319,  ..., -0.0727, -0.7742,  0.2345],\n",
      "         [-0.2190,  0.7717, -0.1616,  ..., -0.0045, -0.5507, -0.4335]],\n",
      "\n",
      "        [[-0.3710,  0.5880, -0.1502,  ..., -0.2938, -0.1979,  0.3746],\n",
      "         [-0.4135,  0.9116,  0.1039,  ..., -0.2464, -0.2319,  0.2671],\n",
      "         [-0.5402,  0.8410,  0.3072,  ..., -0.3392, -0.3055,  0.1072],\n",
      "         ...,\n",
      "         [-0.4916,  0.4242, -0.1777,  ..., -0.3625, -0.1853,  0.4384],\n",
      "         [-0.3994,  0.4076, -0.2245,  ..., -0.2317, -0.1596,  0.3773],\n",
      "         [-0.3991,  0.4054, -0.2113,  ..., -0.2379, -0.1343,  0.3737]]],\n",
      "       grad_fn=<NativeLayerNormBackward0>), hidden_states=None, attentions=None)\n"
     ]
    }
   ],
   "source": [
    "outputs= model(**inputs)\n",
    "print(outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 17, 768])\n"
     ]
    }
   ],
   "source": [
    "print(outputs.last_hidden_state.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 2])\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForSequenceClassification\n",
    "\n",
    "checkpoint = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
    "model = AutoModelForSequenceClassification.from_pretrained(checkpoint)\n",
    "outputs = model(**inputs)\n",
    "\n",
    "print(outputs.logits.shape)   #Since we have just two sentences and two labels, the result we get from our model is of shape 2 x 2."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. PostProcessing the output\n",
    "\n",
    "As an output of the model , we get logits. Logits are raw, unnormalized scores outputted by last laer of the model.\n",
    "\n",
    "We need to convert logits into probabilities score. For that the logits values are passed through softmax layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[9.9905e-01, 9.5343e-04],\n",
      "        [9.9656e-01, 3.4362e-03]], grad_fn=<SoftmaxBackward0>)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "predictions = torch.nn.functional.softmax(outputs.logits, dim=-1)\n",
    "print(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 'NEGATIVE', 1: 'POSITIVE'}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.config.id2label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "My gpu kernel",
   "language": "python",
   "name": "gputest"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
