{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset eli5 (C:/Users/AIXI/.cache/huggingface/datasets/eli5/LFQA_reddit/1.0.0/17574e5502a10f41bbd17beba83e22475b499fa62caa1384a3d093fc856fe6fa)\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "eli5= load_dataset(\"eli5\", split=\"train_asks[:5000]\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "eli5= eli5.train_test_split(test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'q_id': '2fhvmo',\n",
       " 'title': 'Can somebody please explain the difference between a shear thinning and a thixotropic fluid?',\n",
       " 'selftext': \"As far as i'm aware a shear thinning fluid is one that shows a decrease in viscosity with increasing shear stress.\\n\\nHaving looked up the definition of a thixotropic fluid, it seems as though they're the same thing, although from what I've read is seems as though they do a have a subtly different meaning.\\n\\nCould anyone shed some light on what the difference between these two things is? Thanks.\",\n",
       " 'document': '',\n",
       " 'subreddit': 'askscience',\n",
       " 'answers': {'a_id': ['ck9sk4e'],\n",
       "  'text': [\"I think your hangup is not so much that they are very similar, but rather that they are describing **ideal** behaviors that don't really exist independently from each other in real life materials that we know of.\\n\\nShear-thinning/thickening is just a [nonlinear relationship between shear stress and shear strain](_URL_1_); thins when sheared. Simple enough. Thixotropic/rheopectic refers to [time-dependent change in viscosity after some force, such as shear or agitation etc, is applied](_URL_0_).\\n\\nWhat would an ideal material that is shear-thinning but not thixotropic be like? Well, I'd suppose if you applied an infinite impulse function of shear, it would instantly drop viscosity when the shear is applied, then instantly return to its original viscosity when shear stops. That would be how it can be non-thixotropic.\\n\\nOf course, such materials don't exist (that I know of). It takes time to apply shear; it also takes time for materials to do the shear thinning. Since there are no instant response materials, it would be very difficult to isolate the two.\\n\\nI think of it a bit like compressive strength vs hardness - they are obviously different concepts, but it'd be a bit difficult to come up with a soft material that has great compressive strength in real life.\"],\n",
       "  'score': [2]},\n",
       " 'title_urls': {'url': []},\n",
       " 'selftext_urls': {'url': []},\n",
       " 'answers_urls': {'url': ['http://www.dfi.uchile.cl/~rsoto/docencia/FluidosNoNewton2008/trixotropia.pdf',\n",
       "   'http://www.chem.com.au/science/rheology/rheology2/']}}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eli5[\"test\"][22]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# eli5= eli5.flatten()\n",
    "# eli5[\"train\"][22]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i in eli5[\"train\"]:\n",
    "#     print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "\n",
    "def process_dataset(datasett):\n",
    "    # list to store extracted elements\n",
    "    titles = []\n",
    "    selftexts = []\n",
    "    texts = []\n",
    "\n",
    "    for item in datasett:\n",
    "       title = item['title']\n",
    "       selftext = item['selftext']\n",
    "       text = item['answers']['text'][0]\n",
    "\n",
    "       titles.append(title)\n",
    "       selftexts.append(selftext)\n",
    "       texts.append(text)\n",
    "\n",
    "    df = pd.DataFrame({\n",
    "       'Title': titles,\n",
    "       'Selftext': selftexts,\n",
    "       'Text': texts\n",
    "    })\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train= process_dataset(eli5['train'])\n",
    "df_test= process_dataset(eli5['test'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4000\n",
      "1000\n"
     ]
    }
   ],
   "source": [
    "print(len(df_train))\n",
    "print(len(df_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Title</th>\n",
       "      <th>Selftext</th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[Earth Sciences] What are the geological speci...</td>\n",
       "      <td>My searching of the internet and reddit hasn't...</td>\n",
       "      <td>It's tough to say the exact method of formatio...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>How come different allotropes of an element ha...</td>\n",
       "      <td>I have never studied chemistry, but I know tha...</td>\n",
       "      <td>The carbon atoms in diamond and graphite bond ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Is there a scientific explanation for people w...</td>\n",
       "      <td>After digging through /r/deadbedrooms and /r/l...</td>\n",
       "      <td>Although this is probably incredibly more comp...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Ligo gravitational observatory and nuclear bombs?</td>\n",
       "      <td>Could a nuclear bomb trigger ligo? Would we kn...</td>\n",
       "      <td>Yes it would trigger ligo but they back check ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Different ethnicity and their comfort levels i...</td>\n",
       "      <td>I have a diverse group of friends.  Trinidadia...</td>\n",
       "      <td>Why would it have to be genetic? Maybe they ju...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               Title  \\\n",
       "0  [Earth Sciences] What are the geological speci...   \n",
       "1  How come different allotropes of an element ha...   \n",
       "2  Is there a scientific explanation for people w...   \n",
       "3  Ligo gravitational observatory and nuclear bombs?   \n",
       "4  Different ethnicity and their comfort levels i...   \n",
       "\n",
       "                                            Selftext  \\\n",
       "0  My searching of the internet and reddit hasn't...   \n",
       "1  I have never studied chemistry, but I know tha...   \n",
       "2  After digging through /r/deadbedrooms and /r/l...   \n",
       "3  Could a nuclear bomb trigger ligo? Would we kn...   \n",
       "4  I have a diverse group of friends.  Trinidadia...   \n",
       "\n",
       "                                                Text  \n",
       "0  It's tough to say the exact method of formatio...  \n",
       "1  The carbon atoms in diamond and graphite bond ...  \n",
       "2  Although this is probably incredibly more comp...  \n",
       "3  Yes it would trigger ligo but they back check ...  \n",
       "4  Why would it have to be genetic? Maybe they ju...  "
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tokenizer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[38], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39m## preprocessing of EVAL dataset\u001b[39;00m\n\u001b[0;32m      2\u001b[0m eval_text_data\u001b[39m=\u001b[39m df_test[\u001b[39m'\u001b[39m\u001b[39mText\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39mtolist()\n\u001b[1;32m----> 3\u001b[0m tokenized_eval_data\u001b[39m=\u001b[39m tokenizer(eval_text_data, truncation\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, padding\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[0;32m      5\u001b[0m \u001b[39m# creating a TextDataset for evaluation\u001b[39;00m\n\u001b[0;32m      6\u001b[0m eval_dataset\u001b[39m=\u001b[39m TextDataset(tokenizer\u001b[39m=\u001b[39mtokenizer, file_path\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, column_names\u001b[39m=\u001b[39m [\u001b[39m'\u001b[39m\u001b[39minput_ids\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mattention_mask\u001b[39m\u001b[39m'\u001b[39m], data\u001b[39m=\u001b[39mtokenized_eval_data)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'tokenizer' is not defined"
     ]
    }
   ],
   "source": [
    "## preprocessing of EVAL dataset\n",
    "eval_text_data= df_test['Text'].tolist()\n",
    "tokenized_eval_data= tokenizer(eval_text_data, truncation=True, padding=True)\n",
    "\n",
    "# creating a TextDataset for evaluation\n",
    "eval_dataset= TextDataset(tokenizer=tokenizer, file_path=None, column_names= ['input_ids', 'attention_mask'], data=tokenized_eval_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800000; text-decoration-color: #800000\">╭─────────────────────────────── </span><span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">Traceback </span><span style=\"color: #bf7f7f; text-decoration-color: #bf7f7f; font-weight: bold\">(most recent call last)</span><span style=\"color: #800000; text-decoration-color: #800000\"> ────────────────────────────────╮</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #808000; text-decoration-color: #808000\">C:\\Users\\AIXI\\AppData\\Local\\Temp\\ipykernel_8380\\1438674355.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">16</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">&lt;module&gt;</span>                     <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000; font-style: italic\">[Errno 2] No such file or directory: </span>                                                            <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000; font-style: italic\">'C:\\\\Users\\\\AIXI\\\\AppData\\\\Local\\\\Temp\\\\ipykernel_8380\\\\1438674355.py'</span>                           <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">╰──────────────────────────────────────────────────────────────────────────────────────────────────╯</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000; font-weight: bold\">TypeError: </span><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">TextDataset.__init__</span><span style=\"font-weight: bold\">()</span> got an unexpected keyword argument <span style=\"color: #008000; text-decoration-color: #008000\">'column_names'</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[31m╭─\u001b[0m\u001b[31m──────────────────────────────\u001b[0m\u001b[31m \u001b[0m\u001b[1;31mTraceback \u001b[0m\u001b[1;2;31m(most recent call last)\u001b[0m\u001b[31m \u001b[0m\u001b[31m───────────────────────────────\u001b[0m\u001b[31m─╮\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[33mC:\\Users\\AIXI\\AppData\\Local\\Temp\\ipykernel_8380\\1438674355.py\u001b[0m:\u001b[94m16\u001b[0m in \u001b[92m<module>\u001b[0m                     \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[3;31m[Errno 2] No such file or directory: \u001b[0m                                                            \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[3;31m'C:\\\\Users\\\\AIXI\\\\AppData\\\\Local\\\\Temp\\\\ipykernel_8380\\\\1438674355.py'\u001b[0m                           \u001b[31m│\u001b[0m\n",
       "\u001b[31m╰──────────────────────────────────────────────────────────────────────────────────────────────────╯\u001b[0m\n",
       "\u001b[1;91mTypeError: \u001b[0m\u001b[1;35mTextDataset.__init__\u001b[0m\u001b[1m(\u001b[0m\u001b[1m)\u001b[0m got an unexpected keyword argument \u001b[32m'column_names'\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, TrainingArguments, Trainer\n",
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel, TextDataset, DataCollatorForLanguageModeling\n",
    "\n",
    "model_name= \"distilgpt2\"\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
    "model= AutoModelForCausalLM.from_pretrained(model_name)\n",
    "\n",
    "texts= df_train[\"Text\"].tolist()\n",
    "\n",
    "# Add a padding token to the tokenizer\n",
    "tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n",
    "\n",
    "tokenized_data = tokenizer(texts, truncation=True, padding=True, return_tensors=\"pt\", max_length=128)\n",
    "\n",
    "# Create a TextDataset\n",
    "dataset = TextDataset(tokenizer=tokenizer, file_path=None, column_names=['input_ids', 'attention_mask'], data=tokenized_data)\n",
    "\n",
    "# Create a DataCollator for Language Modeling\n",
    "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=\"my_awesome_eli5_clm-model\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    num_train_epochs=3,\n",
    "    learning_rate=2e-5,\n",
    "    weight_decay=0.01,\n",
    "    push_to_hub=False,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=dataset,\n",
    "    eval_dataset=eval_dataset[\"test\"],\n",
    "    data_collator=data_collator,\n",
    "    device=\"cuda\" if torch.cuda.is_available() else \"cpu\",\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel, DataCollatorForLanguageModeling, Trainer, TrainingArguments\n",
    "from datasets import load_dataset\n",
    "\n",
    "# Initialize the GPT-2 tokenizer and model\n",
    "model_name = 'distilgpt2'\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
    "model = GPT2LMHeadModel.from_pretrained(model_name)\n",
    "\n",
    "# Add a padding token to the tokenizer\n",
    "tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n",
    "\n",
    "# Tokenize your text data\n",
    "text_data = df_train['Text'].tolist()\n",
    "tokenized_data = tokenizer(text_data, truncation=True, padding='max_length', return_tensors='pt', max_length=128)\n",
    "\n",
    "# Tokenize evaluation data\n",
    "eval_text_data = df_test['Text'].tolist()\n",
    "tokenized_eval_data = tokenizer(eval_text_data, truncation=True, padding='max_length', return_tensors='pt', max_length=128)\n",
    "\n",
    "# Create datasets using the datasets library\n",
    "train_dataset = {'input_ids': tokenized_data['input_ids'], 'attention_mask': tokenized_data['attention_mask']}\n",
    "eval_dataset = {'input_ids': tokenized_eval_data['input_ids'], 'attention_mask': tokenized_eval_data['attention_mask']}\n",
    "\n",
    "# Create a DataCollator for Language Modeling\n",
    "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./fine-tuned-gpt2',\n",
    "    overwrite_output_dir=True,\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    save_steps=10_000,\n",
    "    save_total_limit=2,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    eval_steps=10_000,\n",
    "    do_eval=True,\n",
    "    logging_steps=100,\n",
    "    evaluation_report=True,\n",
    "    max_seq_length=128,\n",
    ")\n",
    "\n",
    "# Create a Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    data_collator=data_collator,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,  # Add the evaluation dataset here\n",
    "    compute_metrics=None,  # Optional: Define custom evaluation metrics\n",
    "    tokenizer=tokenizer,\n",
    "    device=\"cuda\" if torch.cuda.is_available() else \"cpu\",\n",
    ")\n",
    "\n",
    "# Fine-tune the model\n",
    "trainer.train()\n",
    "\n",
    "# Save the fine-tuned model\n",
    "trainer.save_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def preprocess_function(examples, tokenizer):\n",
    "#     return tokenizer([\" \".join(x) for x in examples[\"answers.text\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenized_eli5= eli5.map(\n",
    "#     preprocess_function(eli5[\"train\"], tokenizer),\n",
    "#     # preprocess_function,\n",
    "#     batched=False,\n",
    "#     # num_proc=4,\n",
    "#     remove_columns=eli5[\"train\"].column_names,\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "# block_size = 128\n",
    "\n",
    "\n",
    "# def group_texts(examples):\n",
    "#     # Concatenate all texts.\n",
    "#     concatenated_examples = {k: sum(examples[k], []) for k in examples.keys()}\n",
    "#     total_length = len(concatenated_examples[list(examples.keys())[0]])\n",
    "#     # We drop the small remainder, we could add padding if the model supported it instead of this drop, you can\n",
    "#     # customize this part to your needs.\n",
    "#     if total_length >= block_size:\n",
    "#         total_length = (total_length // block_size) * block_size\n",
    "#     # Split by chunks of block_size.\n",
    "#     result = {\n",
    "#         k: [t[i : i + block_size] for i in range(0, total_length, block_size)]\n",
    "#         for k, t in concatenated_examples.items()\n",
    "#     }\n",
    "#     result[\"labels\"] = result[\"input_ids\"].copy()\n",
    "#     return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lm_dataset = tokenized_eli5.map(group_texts, batched=True, num_proc=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['q_id', 'title', 'selftext', 'document', 'subreddit', 'answers.a_id', 'answers.text', 'answers.score', 'title_urls.url', 'selftext_urls.url', 'answers_urls.url'],\n",
       "        num_rows: 4000\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['q_id', 'title', 'selftext', 'document', 'subreddit', 'answers.a_id', 'answers.text', 'answers.score', 'title_urls.url', 'selftext_urls.url', 'answers_urls.url'],\n",
       "        num_rows: 1000\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eli5.flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "My gpu kernel",
   "language": "python",
   "name": "gputest"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
