{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset eli5 (C:/Users/AIXI/.cache/huggingface/datasets/eli5/LFQA_reddit/1.0.0/17574e5502a10f41bbd17beba83e22475b499fa62caa1384a3d093fc856fe6fa)\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "eli5= load_dataset(\"eli5\", split=\"train_asks[:5000]\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "eli5= eli5.train_test_split(test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'q_id': '4zt3dd',\n",
       " 'title': 'If air and water are both considered \"fluids\" in Physics sense, why are their applications labeled pneumatic/hydraulic, respectfully, and not just hydraulic?',\n",
       " 'selftext': '',\n",
       " 'document': '',\n",
       " 'subreddit': 'askscience',\n",
       " 'answers': {'a_id': ['d6ykxu3'],\n",
       "  'text': [\"Fluid just means that something flows. The key difference between pneumatics and hydraulics is that while gases are compressible, liquids are not.\\n\\nThe benefits of hydraulics are that they are more responsive and can supply more power than pneumatics, because the liquids don't compress. However, hydraulics are harder to build and maintain, and are more susceptible to shock damage because of the incompressibility.\"],\n",
       "  'score': [20]},\n",
       " 'title_urls': {'url': []},\n",
       " 'selftext_urls': {'url': []},\n",
       " 'answers_urls': {'url': []}}"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eli5[\"train\"][22]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# eli5= eli5.flatten()\n",
    "# eli5[\"train\"][22]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "\n",
    "# Lists to store extracted elements\n",
    "titles = []\n",
    "selftexts = []\n",
    "texts = []\n",
    "\n",
    "# Loop through each data point\n",
    "for data in eli5['train']:\n",
    "    title = data['title']\n",
    "    selftext = data['selftext']\n",
    "    text = data['answers']['text'][0]\n",
    "\n",
    "    titles.append(title)\n",
    "    selftexts.append(selftext)\n",
    "    texts.append(text)\n",
    "\n",
    "# Create a DataFrame\n",
    "df = pd.DataFrame({\n",
    "    'Title': titles,\n",
    "    'Selftext': selftexts,\n",
    "    'Text': texts\n",
    "})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Title</th>\n",
       "      <th>Selftext</th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Do humans really affect the environment all th...</td>\n",
       "      <td>There are the same hurricanes, rainfall, tempe...</td>\n",
       "      <td>[Intergovernmental panel on climate change](_U...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>What happens to the nucleus in the plasma state?</td>\n",
       "      <td>I read from my teacher's lecture notes that wh...</td>\n",
       "      <td>No, the nucleus stays intact in a plasma. It t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>reddit 101</td>\n",
       "      <td>Hello /r/AskScience users! This post is part o...</td>\n",
       "      <td>Very helpful post, thanks for the work it clea...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Why can't we test for neurotransmitter levels ...</td>\n",
       "      <td>It seems as though we have tests available for...</td>\n",
       "      <td>Neurotransmitters generally pop out of one neu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>How true is Tesla's earthquake machine story?</td>\n",
       "      <td></td>\n",
       "      <td>It's plausible.  He used some very high power ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               Title  \\\n",
       "0  Do humans really affect the environment all th...   \n",
       "1   What happens to the nucleus in the plasma state?   \n",
       "2                                         reddit 101   \n",
       "3  Why can't we test for neurotransmitter levels ...   \n",
       "4      How true is Tesla's earthquake machine story?   \n",
       "\n",
       "                                            Selftext  \\\n",
       "0  There are the same hurricanes, rainfall, tempe...   \n",
       "1  I read from my teacher's lecture notes that wh...   \n",
       "2  Hello /r/AskScience users! This post is part o...   \n",
       "3  It seems as though we have tests available for...   \n",
       "4                                                      \n",
       "\n",
       "                                                Text  \n",
       "0  [Intergovernmental panel on climate change](_U...  \n",
       "1  No, the nucleus stays intact in a plasma. It t...  \n",
       "2  Very helpful post, thanks for the work it clea...  \n",
       "3  Neurotransmitters generally pop out of one neu...  \n",
       "4  It's plausible.  He used some very high power ...  "
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts= df[\"Text\"].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "tokenizer= AutoTokenizer.from_pretrained(\"distilgpt2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.add_special_tokens({'pad_token': '[PAD]'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize the text data\n",
    "tokenized_texts = tokenizer(texts, truncation=True, padding=True, return_tensors=\"pt\", max_length=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[   58,  9492, 31353,  ...,   338,  4737,   329],\n",
      "        [ 2949,    11,   262,  ..., 50257, 50257, 50257],\n",
      "        [16371,  7613,  1281,  ..., 50257, 50257, 50257],\n",
      "        ...,\n",
      "        [ 1858,   338,   257,  ..., 50257, 50257, 50257],\n",
      "        [  464,   411,   513,  ..., 50257, 50257, 50257],\n",
      "        [ 1639,   423,  3294,  ...,   284,   307,  1498]])\n",
      "4000\n"
     ]
    }
   ],
   "source": [
    "print(tokenized_texts['input_ids'])\n",
    "print(len(tokenized_texts['input_ids']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def preprocess_function(examples, tokenizer):\n",
    "#     return tokenizer([\" \".join(x) for x in examples[\"answers.text\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenized_eli5= eli5.map(\n",
    "#     preprocess_function(eli5[\"train\"], tokenizer),\n",
    "#     # preprocess_function,\n",
    "#     batched=False,\n",
    "#     # num_proc=4,\n",
    "#     remove_columns=eli5[\"train\"].column_names,\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "# block_size = 128\n",
    "\n",
    "\n",
    "# def group_texts(examples):\n",
    "#     # Concatenate all texts.\n",
    "#     concatenated_examples = {k: sum(examples[k], []) for k in examples.keys()}\n",
    "#     total_length = len(concatenated_examples[list(examples.keys())[0]])\n",
    "#     # We drop the small remainder, we could add padding if the model supported it instead of this drop, you can\n",
    "#     # customize this part to your needs.\n",
    "#     if total_length >= block_size:\n",
    "#         total_length = (total_length // block_size) * block_size\n",
    "#     # Split by chunks of block_size.\n",
    "#     result = {\n",
    "#         k: [t[i : i + block_size] for i in range(0, total_length, block_size)]\n",
    "#         for k, t in concatenated_examples.items()\n",
    "#     }\n",
    "#     result[\"labels\"] = result[\"input_ids\"].copy()\n",
    "#     return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lm_dataset = tokenized_eli5.map(group_texts, batched=True, num_proc=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['q_id', 'title', 'selftext', 'document', 'subreddit', 'answers.a_id', 'answers.text', 'answers.score', 'title_urls.url', 'selftext_urls.url', 'answers_urls.url'],\n",
       "        num_rows: 4000\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['q_id', 'title', 'selftext', 'document', 'subreddit', 'answers.a_id', 'answers.text', 'answers.score', 'title_urls.url', 'selftext_urls.url', 'answers_urls.url'],\n",
       "        num_rows: 1000\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eli5.flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "My gpu kernel",
   "language": "python",
   "name": "gputest"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
